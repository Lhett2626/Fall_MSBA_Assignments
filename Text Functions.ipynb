{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Functions\n",
    "\n",
    "Most of the interesting work in this assignment will happen when you create your own file `text_functions.py`. This next cell will load the higher-level functions into the kernel. \n",
    "\n",
    "**Note**: When you submit this, leave the output of code _I've_ written printed to the screen. This will make it easier for me to check your work. If you print some large stuff to the screen, you can delete those cells or just suppress the printing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters\n",
    "from text_functions import *\n",
    "import nltk\n",
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll just test them out. We'll use information from the Reuters corpus. More information can be found [here](https://www.nltk.org/book/ch02.html) in section 1.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = reuters.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_cats = [\"barley\",\"corn\",\"cotton\",\"grain\",\"potato\",\"rye\",\"sugar\",\"wheat\"]\n",
    "mining_cats = [\"alum\",\"copper\",\"silver\",\"gold\",\"iron-steel\",\"tin\",\"zinc\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Reuters corpus has 1.3M articles arranged into these categories. Let's build some big sets of text based on these categories. Articles can be in multiple categories. (Quick: what type of corpus do we call that?) So we'll pull articles that are exclusively in one of our categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_articles = set()\n",
    "mining_articles = set()\n",
    "\n",
    "for cat in crop_cats : \n",
    "    for article in reuters.fileids(cat) : \n",
    "        crop_articles.add(article)\n",
    "        \n",
    "for cat in mining_cats : \n",
    "    for article in reuters.fileids(cat) : \n",
    "        mining_articles.add(article)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_both = crop_articles.intersection(mining_articles)\n",
    "crop_articles = crop_articles - in_both\n",
    "mining_articles = mining_articles - in_both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_text = []\n",
    "mining_text = []\n",
    "\n",
    "for article in crop_articles :\n",
    "    # Categories are stored in the article in upper case\n",
    "    article_text = [w for w in reuters.words(article) if w != w.upper()]\n",
    "    crop_text.extend(article_text)\n",
    "\n",
    "for article in mining_articles :\n",
    "    # Categories are stored in the article in upper case\n",
    "    article_text = [w for w in reuters.words(article) if w != w.upper()]\n",
    "    mining_text.extend(article_text)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're in a position to test our code! \n",
    "\n",
    "### Cleaning and Tokenizing\n",
    "\n",
    "First we'll clean and tokenize, sending one set of text in as a list and the other in as a string, just to make sure both options work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "holder = crop_text\n",
    "crop_text = clean_tokenize(holder)\n",
    "crop_text_2 = clean_tokenize(holder,remove_sw=False,remove_non_alpha=False)\n",
    "mining_text = clean_tokenize(\" \".join(mining_text),remove_sw=True,lowercase=True,remove_non_alpha=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed all assertion tests!\n"
     ]
    }
   ],
   "source": [
    "assert(len(crop_text)==69727)\n",
    "assert(len(mining_text)==31275)\n",
    "assert(len([w for w in crop_text if w != w.lower()])==0)\n",
    "assert(len([w for w in mining_text if w != w.lower()])==0)\n",
    "assert(len(crop_text_2) - len(crop_text)==42870)\n",
    "print(\"Passed all assertion tests!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patterns in a Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patterns(text,num_words=10)  :\n",
    "    \"\"\"Computes basic statistics on a text corpus. \n",
    "    \n",
    "       This function takes text as an input and returns a dictionary of statistics,\n",
    "       after cleaning the text. \n",
    "       \n",
    "       Args: \n",
    "           text: a list of tokens. Calls `clean_tokenize` on the text.\n",
    "           num_words: Number of words to include in the FreqDist object\n",
    "           in the results. Defaults to 10. \n",
    "           \n",
    "       Returns: \n",
    "           A dictionary with the following keys: \n",
    "           * tokens\n",
    "           * unique_tokens\n",
    "           * avg_token_length\n",
    "           * lexical_diversity\n",
    "           * top_words: The value is a result of a call to \n",
    "             FreqDist(text).most_common(num_words)\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    if(len(text)==0) :\n",
    "        raise ValueError(\"Can't work with empty text object.\")\n",
    "    else :\n",
    "        text = clean_tokenize(text)\n",
    "\n",
    "    # Calculate total tokens\n",
    "    total_tokens = len(text)\n",
    "    \n",
    "    # Calculating unique tokens\n",
    "    unique_tokens = len(list(FreqDist(text).keys()))\n",
    "\n",
    "    #Calculating average token length\n",
    "    avg_token_len = sum([len(word) for word in text]) / len(text)\n",
    "    \n",
    "    #Calculating lexical diversity\n",
    "    lex_diversity = unique_tokens/total_tokens\n",
    "    \n",
    "    #Calculating top 10 \n",
    "    top_words = FreqDist(text).most_common(num_words)\n",
    "        \n",
    "    \n",
    "    #Results to a dictionary\n",
    "    results = {'tokens':total_tokens,\n",
    "               'unique_tokens':unique_tokens,\n",
    "               'avg_token_length':avg_token_len,\n",
    "               'lexical_diversity':lex_diversity,\n",
    "               'top_words':top_words}\n",
    "    \n",
    "    return(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': 69727,\n",
       " 'unique_tokens': 6682,\n",
       " 'avg_token_length': 6.191073759089019,\n",
       " 'lexical_diversity': 0.09583088330202073,\n",
       " 'top_words': [('said', 2323),\n",
       "  ('tonnes', 1462),\n",
       "  ('mln', 1291),\n",
       "  ('wheat', 813),\n",
       "  ('year', 661),\n",
       "  ('sugar', 592),\n",
       "  ('pct', 529),\n",
       "  ('grain', 523),\n",
       "  ('would', 487),\n",
       "  ('last', 474)]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_patterns(crop_text,num_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': 31275,\n",
       " 'unique_tokens': 4656,\n",
       " 'avg_token_length': 6.127769784172662,\n",
       " 'lexical_diversity': 0.14887290167865708,\n",
       " 'top_words': [('said', 1171),\n",
       "  ('gold', 444),\n",
       "  ('mln', 321),\n",
       "  ('pct', 311),\n",
       "  ('year', 301),\n",
       "  ('tonnes', 260),\n",
       "  ('dlrs', 236),\n",
       "  ('company', 210),\n",
       "  ('lt', 208),\n",
       "  ('copper', 206)]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_patterns(mining_text,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_texts(corpus_1, corpus_2, num_words = 10, ratio_cutoff = 5):\n",
    "\n",
    "#error check and cleaning\n",
    "    c1_results = get_patterns(corpus_1)\n",
    "    c2_results = get_patterns(corpus_2)\n",
    "    \n",
    "    #ratio cutoff = 5\n",
    "    #ratio cutoff is the number of words in each dataset.\n",
    "    c1_dict = {}\n",
    "    for key, value in FreqDist(corpus_1).items():\n",
    "        if value > ratio_cutoff:\n",
    "            c1_dict[key] = value/len(corpus_1)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    c2_dict = {}\n",
    "    for key, value in FreqDist(corpus_2).items():\n",
    "        if value > ratio_cutoff:\n",
    "            c2_dict[key] = value/len(corpus_1)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    #print(c1_dict)\n",
    "    #print(c2_dict)\n",
    "\n",
    "    c_1VSc_2 = {}\n",
    "    c_2VSc_1 = {}\n",
    "    \n",
    "    #adding comparison to dictionaries\n",
    "    for key in c1_dict.keys():\n",
    "        if key in c2_dict.keys():\n",
    "            c_1VSc_2[key] = c1_dict[key]/c2_dict[key]\n",
    "            c_2VSc_1[key] = c2_dict[key]/c1_dict[key]\n",
    "    \n",
    "    #sorting dictionary\n",
    "    #num words is the number of words to be displayed\n",
    "    one_vs_two = sorted(c_1VSc_2.items(), key=lambda k: k[1], reverse = True)[:num_words]\n",
    "    two_vs_one = sorted(c_2VSc_1.items(), key=lambda k: k[1], reverse = True)[:num_words]\n",
    "    \n",
    "    #Adding to dictionary\n",
    "    comparison_dict = {'one': c1_results,\n",
    "                      'two': c2_results,\n",
    "                      'one_vs_two': one_vs_two,\n",
    "                      'two_vs_one': two_vs_one}\n",
    "    \n",
    "    return(comparison_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'one': {'tokens': 69727,\n",
       "  'unique_tokens': 6682,\n",
       "  'avg_token_length': 6.191073759089019,\n",
       "  'lexical_diversity': 0.09583088330202073,\n",
       "  'top_words': [('said', 2323),\n",
       "   ('tonnes', 1462),\n",
       "   ('mln', 1291),\n",
       "   ('wheat', 813),\n",
       "   ('year', 661),\n",
       "   ('sugar', 592),\n",
       "   ('pct', 529),\n",
       "   ('grain', 523),\n",
       "   ('would', 487),\n",
       "   ('last', 474)]},\n",
       " 'two': {'tokens': 31275,\n",
       "  'unique_tokens': 4656,\n",
       "  'avg_token_length': 6.127769784172662,\n",
       "  'lexical_diversity': 0.14887290167865708,\n",
       "  'top_words': [('said', 1171),\n",
       "   ('gold', 444),\n",
       "   ('mln', 321),\n",
       "   ('pct', 311),\n",
       "   ('year', 301),\n",
       "   ('tonnes', 260),\n",
       "   ('dlrs', 236),\n",
       "   ('company', 210),\n",
       "   ('lt', 208),\n",
       "   ('copper', 206)]},\n",
       " 'one_vs_two': [('nil', 34.300000000000004),\n",
       "  ('soviet', 21.5),\n",
       "  ('department', 21.3125),\n",
       "  ('previous', 14.285714285714286),\n",
       "  ('vs', 14.0),\n",
       "  ('offer', 13.300000000000002),\n",
       "  ('administration', 12.285714285714285),\n",
       "  ('french', 12.125),\n",
       "  ('area', 11.300000000000002),\n",
       "  ('private', 10.833333333333334)],\n",
       " 'two_vs_one': [('lt', 11.555555555555555),\n",
       "  ('reserves', 9.875),\n",
       "  ('ltd', 9.214285714285714),\n",
       "  ('lead', 6.1000000000000005),\n",
       "  ('capacity', 5.6000000000000005),\n",
       "  ('company', 4.883720930232558),\n",
       "  ('strike', 4.083333333333334),\n",
       "  ('zambia', 3.857142857142857),\n",
       "  ('fire', 3.666666666666667),\n",
       "  ('plants', 3.5714285714285716)]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_texts(crop_text,mining_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "WORDS = Counter(words(open('big.txt').read()))\n",
    "\n",
    "def P(word, N=sum(WORDS.values())): \n",
    "    \"Probability of `word`.\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(isinstance(crop_text,(list)))\n",
    "corrected_words = dict()\n",
    "\n",
    "for word in crop_text[:1000] :\n",
    "    cw = correction(word)\n",
    "    if cw != word :\n",
    "        corrected_words[word] = cw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corrected_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mln was corrected to man\n",
      "dlrs was corrected to days\n",
      "earmarked was corrected to remarked\n",
      "exporters was corrected to exports\n",
      "tonnes was corrected to tones\n",
      "iraq was corrected to ran\n",
      "algeria was corrected to algebra\n",
      "paddy was corrected to daddy\n",
      "hectares was corrected to hectare\n",
      "milled was corrected to killed\n",
      "portland was corrected to poland\n",
      "kan was corrected to an\n",
      "reagan was corrected to began\n",
      "reps was corrected to rep\n",
      "minn was corrected to mind\n",
      "dorgan was corrected to organ\n",
      "pct was corrected to put\n",
      "generic was corrected to genetic\n",
      "alfredo was corrected to alfred\n",
      "ricart was corrected to cart\n",
      "constantin was corrected to constantine\n",
      "reuters was corrected to renters\n",
      "pact was corrected to part\n",
      "allocations was corrected to allocation\n",
      "souffle was corrected to scuffle\n",
      "fob was corrected to for\n",
      "graniere was corrected to grangers\n",
      "companie was corrected to companies\n",
      "miguel was corrected to michel\n",
      "braceras was corrected to braces\n",
      "resende was corrected to presence\n",
      "sao was corrected to so\n",
      "paulo was corrected to paul\n",
      "argentine was corrected to argentina\n",
      "importer was corrected to imported\n",
      "discounts was corrected to discount\n",
      "thai was corrected to that\n",
      "sen was corrected to men\n",
      "rudy was corrected to rude\n",
      "conceivably was corrected to conceivable\n",
      "leahy was corrected to leafy\n",
      "vt was corrected to it\n",
      "rebate was corrected to debate\n",
      "ecus was corrected to eyes\n",
      "kilos was corrected to kills\n",
      "resell was corrected to retell\n",
      "oilseed was corrected to oiled\n",
      "sanctions was corrected to sanction\n",
      "fiji was corrected to fili\n",
      "fijian was corrected to midian\n",
      "expires was corrected to empires\n",
      "reporters was corrected to reporter\n",
      "suva was corrected to such\n",
      "rabuka was corrected to rebuke\n",
      "staged was corrected to stage\n"
     ]
    }
   ],
   "source": [
    "for w, cw in corrected_words.items() :\n",
    "    print(f\"{w} was corrected to {cw}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
