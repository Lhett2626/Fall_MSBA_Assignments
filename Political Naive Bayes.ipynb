{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes on Political Text\n",
    "\n",
    "In this notebook we use Naive Bayes to explore and classify political data. See the `README.md` for full details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "convention_db = sqlite3.connect(\"2020_Conventions.db\")\n",
    "convention_cur = convention_db.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Naive Bayes\n",
    "\n",
    "We'll first build a NB model on the convention data itself, as a way to understand what words distinguish between the two parties. This is analogous to what we did in the \"Comparing Groups\" class work. First, pull in the text \n",
    "for each party and prepare it for use in Naive Bayes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "convention_data = []\n",
    "\n",
    "query_results = convention_cur.execute(\n",
    "                            '''\n",
    "                            SELECT text, party\n",
    "                            FROM conventions\n",
    "                            WHERE speaker != \"Unknown\"\n",
    "                            ''')\n",
    "\n",
    "for row in query_results:\n",
    "    text,party = row \n",
    "\n",
    "    text = [w.lower() for w in text.split()] #lower\n",
    "    text = [re.sub(r'[^\\w\\s]','',w) for w in text] #removing non-alphas\n",
    "    \n",
    "    convention_data.append([text,party])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some random entries and see if they look right. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.choices(convention_data,k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If that looks good, we now need to make our function to turn these into features. In my solution, I wanted to keep the number of features reasonable, so I only used words that occur at least `word_cutoff` times. Here's the code to test that if you want it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With a word cutoff of 5, we have 2350 as features in the model.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "sw = stopwords.words('english')\n",
    "\n",
    "\n",
    "word_cutoff = 5\n",
    "\n",
    "tokens = [w for t, p in convention_data for w in t]\n",
    "\n",
    "word_dist = nltk.FreqDist(tokens)\n",
    "\n",
    "feature_words = set()\n",
    "\n",
    "for word, count in word_dist.items() : \n",
    "    if word in sw: #removing stop words\n",
    "        continue\n",
    "    elif count > word_cutoff: #ensuring it meets word cutoff\n",
    "        feature_words.add(word)\n",
    "        \n",
    "print(f\"With a word cutoff of {word_cutoff}, we have {len(feature_words)} as features in the model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_features(text,fw) :\n",
    "    \n",
    "    ret_dict = dict() #dictionary\n",
    "    if type(text) == str: #Allowing strings to be used\n",
    "        text = text.split()\n",
    "    for word in text:\n",
    "        if word in fw:\n",
    "            ret_dict[word] = True #creating feature words dictionary\n",
    "    \n",
    "    return(ret_dict)\n",
    "    \n",
    "    \"\"\"Given some text, this returns a dictionary holding the\n",
    "       feature words.\n",
    "       \n",
    "       Args: \n",
    "            * text: a piece of text in a continuous string. Assumes\n",
    "            text has been cleaned and case folded.\n",
    "            * fw: the *feature words* that we're considering. A word \n",
    "            in `text` must be in fw in order to be returned. This \n",
    "            prevents us from considering very rarely occurring words.\n",
    "        \n",
    "       Returns: \n",
    "            A dictionary with the words in `text` that appear in `fw`. \n",
    "            Words are only counted once. \n",
    "            If `text` were \"quick quick brown fox\" and `fw` = {'quick','fox','jumps'},\n",
    "            then this would return a dictionary of \n",
    "            {'quick' : True,\n",
    "             'fox' :    True}\n",
    "        \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(feature_words)>0)\n",
    "assert(conv_features(tuple(\"donald is the president\".split()),feature_words)==\n",
    "       {'donald':True,'president':True})\n",
    "assert(conv_features(tuple(\"people are american in america\".split()),feature_words)==\n",
    "                     {'america':True,'american':True,\"people\":True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll build our feature set. Out of curiosity I did a train/test split to see how accurate the classifier was, but we don't strictly need to since this analysis is exploratory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(conv_features(text,feature_words), party) for (text, party) in convention_data] #feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(20201013)\n",
    "random.shuffle(featuresets)\n",
    "\n",
    "test_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.528\n"
     ]
    }
   ],
   "source": [
    "test_set, train_set = featuresets[:test_size], featuresets[test_size:]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set) # fitting the model\n",
    "print(nltk.classify.accuracy(classifier, test_set)) #accuracy of model (The model is not very accurate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                 radical = True           Republ : Democr =     43.0 : 1.0\n",
      "             enforcement = True           Republ : Democr =     33.4 : 1.0\n",
      "                   votes = True           Democr : Republ =     26.2 : 1.0\n",
      "                    mike = True           Republ : Democr =     23.9 : 1.0\n",
      "                   media = True           Republ : Democr =     20.7 : 1.0\n",
      "                   china = True           Republ : Democr =     17.5 : 1.0\n",
      "                 destroy = True           Republ : Democr =     16.5 : 1.0\n",
      "                  defund = True           Republ : Democr =     14.3 : 1.0\n",
      "                    flag = True           Republ : Democr =     14.3 : 1.0\n",
      "                    isis = True           Republ : Democr =     14.3 : 1.0\n",
      "                patriots = True           Republ : Democr =     13.3 : 1.0\n",
      "                 chinese = True           Republ : Democr =     12.2 : 1.0\n",
      "               countries = True           Republ : Democr =     12.2 : 1.0\n",
      "                  lowest = True           Republ : Democr =     12.2 : 1.0\n",
      "               terrorist = True           Republ : Democr =     12.2 : 1.0\n",
      "                 violent = True           Republ : Democr =     12.2 : 1.0\n",
      "               delivered = True           Republ : Democr =     11.8 : 1.0\n",
      "                 abraham = True           Republ : Democr =     11.1 : 1.0\n",
      "            conservative = True           Republ : Democr =     11.1 : 1.0\n",
      "               countless = True           Republ : Democr =     11.1 : 1.0\n",
      "                   crime = True           Republ : Democr =     11.1 : 1.0\n",
      "                   moved = True           Republ : Democr =     11.1 : 1.0\n",
      "                   trade = True           Republ : Democr =     11.1 : 1.0\n",
      "                 freedom = True           Republ : Democr =     10.8 : 1.0\n",
      "                 endless = True           Republ : Democr =     10.1 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(25) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a little prose here about what you see in the classifier. Anything odd or interesting?\n",
    "\n",
    "### My Observations\n",
    "\n",
    "It is interesting that the Republican party seems to use their particular feature words more. Most of the informative features describe Republican feature words. The only democratic word that appeared in the Most Informative Features column was votes. Republican had the other 24 Most Informative feature words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Classifying Congressional Tweets\n",
    "\n",
    "In this part we apply the classifer we just built to a set of tweets by people running for congress\n",
    "in 2018. These tweets are stored in the database `congressional_data.db`. That DB is funky, so I'll\n",
    "give you the query I used to pull out the tweets. Note that this DB has some big tables and \n",
    "is unindexed, so the query takes a minute or two to run on my machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cong_db = sqlite3.connect(\"congressional_data.db\")\n",
    "cong_cur = cong_db.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cong_cur.execute(\n",
    "        '''\n",
    "           SELECT DISTINCT \n",
    "                  cd.candidate, \n",
    "                  cd.party,\n",
    "                  tw.tweet_text\n",
    "           FROM candidate_data cd \n",
    "           INNER JOIN tweets tw ON cd.twitter_handle = tw.handle \n",
    "               AND cd.candidate == tw.candidate \n",
    "               AND cd.district == tw.district\n",
    "           WHERE cd.party in ('Republican','Democratic') \n",
    "               AND tw.tweet_text NOT LIKE '%RT%'\n",
    "        ''')\n",
    "results = list(results) # Just to store it, since the query is time consuming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data = []\n",
    "from nltk.corpus import stopwords\n",
    "sw = stopwords.words('english')\n",
    "\n",
    "\n",
    "\n",
    "for name, party, tweet in results:\n",
    "    tweet = tweet.decode('utf-8') #decoding tweets\n",
    "    tweet = tweet.lower() #lower\n",
    "    tweet = tweet.split() #spliting list\n",
    "    tweet = [word for word in tweet if word.isalpha()] #removing none alpha\n",
    "    tweet = [word for word in tweet if word not in sw] #removing stop words\n",
    "    tweet_data.append([tweet,party]) #appending to dictionary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of tweets here. Let's take a random sample and see how our classifer does. I'm guessing it won't be too great given the performance on the convention speeches..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(20201014)\n",
    "\n",
    "tweet_data_sample = random.choices(tweet_data,k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's our (cleaned) tweet: earlier spoke house floor abt protecting health care women praised work central\n",
      "Actual party is Democratic and our classifer says Republican.\n",
      "\n",
      "Here's our (cleaned) tweet: go\n",
      "Actual party is Democratic and our classifer says Democratic.\n",
      "\n",
      "Here's our (cleaned) tweet: trump thinks easy students overwhelmed crushing burden debt pay student loans\n",
      "Actual party is Democratic and our classifer says Republican.\n",
      "\n",
      "Here's our (cleaned) tweet: grateful first rescue volunteers working tirelessly keep people provide putting lives\n",
      "Actual party is Republican and our classifer says Republican.\n",
      "\n",
      "Here's our (cleaned) tweet: make even greater\n",
      "Actual party is Republican and our classifer says Republican.\n",
      "\n",
      "Here's our (cleaned) tweet: tie series\n",
      "Actual party is Democratic and our classifer says Democratic.\n",
      "\n",
      "Here's our (cleaned) tweet: congrats new gig sd city glad continue\n",
      "Actual party is Democratic and our classifer says Republican.\n",
      "\n",
      "Here's our (cleaned) tweet: really raised toward match right majors room help us get\n",
      "Actual party is Democratic and our classifer says Republican.\n",
      "\n",
      "Here's our (cleaned) tweet: comment period plan expand offshore drilling opened days march share oppose proposed program directly trump comments made email\n",
      "Actual party is Democratic and our classifer says Republican.\n",
      "\n",
      "Here's our (cleaned) tweet: celebrated years eastside commitment saluted community leaders last awards\n",
      "Actual party is Democratic and our classifer says Republican.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for tweet, party in tweet_data_sample :\n",
    "    estimated_party = classifier.classify(conv_features(tweet,feature_words)) #get the classification for tweets\n",
    "    \n",
    "    print(f\"Here's our (cleaned) tweet: {' '.join(tweet)}\")\n",
    "    print(f\"Actual party is {party} and our classifer says {estimated_party}.\")\n",
    "    print(\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've looked at it some, let's score a bunch and see how we're doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "parties = ['Republican','Democratic']\n",
    "results = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for p in parties:\n",
    "    for p1 in parties:\n",
    "        results[p][p1] = 0\n",
    "\n",
    "\n",
    "num_to_score = 10000\n",
    "random.shuffle(tweet_data)\n",
    "\n",
    "for idx, tp in enumerate(tweet_data):\n",
    "    tweet, party = tp    \n",
    "\n",
    "    estimated_party = classifier.classify(conv_features(tweet,feature_words)) #changed to get classification of tweets\n",
    "    \n",
    "    results[party][estimated_party] += 1\n",
    "    \n",
    "    if idx > num_to_score: \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {'Republican': defaultdict(int,\n",
       "                         {'Republican': 3495, 'Democratic': 783}),\n",
       "             'Democratic': defaultdict(int,\n",
       "                         {'Republican': 4561, 'Democratic': 1163})})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflections\n",
    "\n",
    "Our Classification model seems to lean towards democratic classification. More tweets from Republicans were classified as Democratic than Republican. Overall, the classification model is not very good.\n",
    "\n",
    "There could be some potential reasons for this. We are basing our feature word set off convention speeches. Although Democrats and Republicans may have feature words that indicate a party affiliation, convention speech featured words are likely different than tweets. In a convention speech a candite is not limited on the number of characters like they are on twitter. This is just one of the many differences between tweets and convention speeches. A better model would base feature words on a similar corpus type or preferably the same corpus.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
